{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942234b5",
   "metadata": {},
   "source": [
    "# Clustering of Sentinel-2 Timeseries Zonal Mean\n",
    "\n",
    "In this notebook, you will conduct a kernel k means clustering algorithm on a time series of satellite remote sensing data. The time series data can be zonal statistics from OpenET ET rasters or Sentinel-2 vegetation index data. The zones for calculating the zonal statistics are determined by the LandIQ field polygons.\n",
    "\n",
    "This notebook uses the tslearn package for doing the kernel k means clustering. See the tslearn documentation for more details: https://tslearn.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "814b816a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\KDRECH~1\\AppData\\Local\\Temp/ipykernel_86948/238890263.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgeopandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from scipy.optimize import curve_fit\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import KernelKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c47e9e",
   "metadata": {},
   "source": [
    "The following section shows the implementation of the time series clustering on zonal stats satellite remote sensing data in object-oriented programming style. The class TimeSeries Analysis line defines an instance of TimeSeries Analysis that conveys the user-selected parameters for each run of the time series clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13873ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalysis:\n",
    "    def __init__(self, timeseries, landiq, sentinel, class_columns, subclass_columns, class_values, subclass_values, startdate, enddate, varColumn, idColumn, dateColumn):\n",
    "        self.timeseries = timeseries\n",
    "        self.landiq = landiq\n",
    "        self.sentinel = sentinel\n",
    "        self.class_columns = class_columns\n",
    "        self.subclass_columns = subclass_columns\n",
    "        self.class_values = class_values\n",
    "        self.subclass_values = subclass_values\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        self.varColumn = varColumn\n",
    "        self.idColumn = idColumn\n",
    "        self.dateColumn = dateColumn\n",
    "        self.ids = None\n",
    "        self.reshaped = None\n",
    "        self.clusters = None\n",
    "        self.pivoted = None\n",
    "        self.Xtrain = None\n",
    "        self.results = None\n",
    "\n",
    "    def reshaping(self, cloudydates):\n",
    "        \n",
    "        # Filter the LandIQ data by the specified crop class \n",
    "        class_conditions = []\n",
    "        for column in self.class_columns:\n",
    "            class_conditions.append(self.landiq[column].isin(self.class_values))\n",
    "        combined_class_condition = pd.concat(class_conditions, axis=1).any(axis=1)    \n",
    "        if self.subclass_values:\n",
    "            subclass_conditions = []\n",
    "            for column in self.subclass_columns:\n",
    "                subclass_conditions.append(self.landiq[column].isin(self.subclass_values))\n",
    "            combined_subclass_condition = pd.concat(subclass_conditions, axis=1).any(axis=1)\n",
    "            self.landiq = self.landiq[combined_class_condition & combined_subclass_condition]\n",
    "        else:\n",
    "            self.landiq = self.landiq[combined_class_condition]\n",
    "            \n",
    "        # Find the unique LandIQ field IDs for the specified crop class\n",
    "        self.ids = pd.unique(self.landiq[self.idColumn])\n",
    "\n",
    "        # Convert the date column to datetime in timeseries data\n",
    "        self.timeseries[self.dateColumn] = pd.to_datetime(self.timeseries[self.dateColumn])\n",
    "        \n",
    "        # Filter the timeseries by dates\n",
    "        self.timeseries = self.timeseries[(self.timeseries[self.dateColumn] >= self.startdate) & (self.timeseries[self.dateColumn] <= self.enddate)]\n",
    "       \n",
    "        # Filter the timeseries by the LandIQ field IDs for the specified crop class\n",
    "        self.timeseries = self.timeseries[self.timeseries[self.idColumn].isin(self.ids)]\n",
    "\n",
    "        # Reshape for clustering\n",
    "        self.timeseries[self.dateColumn] = pd.to_datetime(self.timeseries[self.dateColumn])\n",
    "        self.timeseries.sort_values(self.dateColumn)\n",
    "        self.pivoted = pd.pivot_table(self.timeseries, index=self.idColumn, columns=self.dateColumn, values=self.varColumn, aggfunc=list)\n",
    "        self.pivoted.dropna(inplace=True)\n",
    "        ind = self.pivoted.index\n",
    "        self.reshaped = np.array(self.pivoted.values.tolist())\n",
    "        ind = np.array(ind.values.tolist())\n",
    "\n",
    "    def run_analysis(self, num_clusters):   \n",
    "\n",
    "        # Filter the LandIQ data by the specified crop class \n",
    "        class_conditions = []\n",
    "        for column in self.class_columns:\n",
    "            class_conditions.append(self.landiq[column].isin(self.class_values))\n",
    "        combined_class_condition = pd.concat(class_conditions, axis=1).any(axis=1)    \n",
    "        if self.subclass_values:\n",
    "            subclass_conditions = []\n",
    "            for column in self.subclass_columns:\n",
    "                subclass_conditions.append(self.landiq[column].isin(self.subclass_values))\n",
    "            combined_subclass_condition = pd.concat(subclass_conditions, axis=1).any(axis=1)\n",
    "            self.landiq = self.landiq[combined_class_condition & combined_subclass_condition]\n",
    "        else:\n",
    "            self.landiq = self.landiq[combined_class_condition]\n",
    "            \n",
    "        # Find all the combinations of crop classes across the 4 growth stages\n",
    "        class_combos = pd.unique(self.landiq['LIQ_REPORT'])\n",
    "        mainclass_combos = pd.unique(self.landiq['MAIN_CROP'])\n",
    "\n",
    "        # Find the unique LandIQ field IDs for the specified crop class\n",
    "        self.ids = pd.unique(self.landiq[self.idColumn])\n",
    "\n",
    "        # Convert the date column to datetime in timeseries data\n",
    "        self.timeseries[self.dateColumn] = pd.to_datetime(self.timeseries[self.dateColumn])\n",
    "        \n",
    "        # Filter the timeseries by dates and exclude additional dates\n",
    "        self.timeseries = self.timeseries[(self.timeseries[self.dateColumn] >= self.startdate) & (self.timeseries[self.dateColumn] <= self.enddate)]\n",
    "        \n",
    "        # Filter the timeseries by the LandIQ field IDs for the specified crop class\n",
    "        self.timeseries = self.timeseries[self.timeseries[self.idColumn].isin(self.ids)]\n",
    "        \n",
    "        # Reshape for clustering\n",
    "        self.timeseries[self.dateColumn] = pd.to_datetime(self.timeseries[self.dateColumn])\n",
    "        self.timeseries.sort_values(self.dateColumn)\n",
    "        self.pivoted = pd.pivot_table(self.timeseries, index=self.idColumn, columns=self.dateColumn, values=self.varColumn, aggfunc=list)\n",
    "        self.pivoted.dropna(inplace=True)\n",
    "        ind = self.pivoted.index\n",
    "        self.reshaped = np.array(self.pivoted.values.tolist())\n",
    "        ind = np.array(ind.values.tolist())\n",
    "\n",
    "        # Apply Kernel KMeans\n",
    "        self.Xtrain = self.reshaped\n",
    "        seed = 0\n",
    "        np.random.seed(seed)\n",
    "        permutation = np.random.permutation(len(self.Xtrain))\n",
    "        self.Xtrain = self.Xtrain[permutation]\n",
    "        ind = ind[permutation]\n",
    "        self.Xtrain = TimeSeriesScalerMeanVariance().fit_transform(self.Xtrain)\n",
    "        gak_km = KernelKMeans(n_clusters=num_clusters, kernel=\"gak\", kernel_params={\"sigma\": \"auto\"}, n_init=20, verbose=True, random_state=seed)\n",
    "        self.clusters = gak_km.fit_predict(self.Xtrain)\n",
    "        self.results = pd.DataFrame({'UniqueID': ind, 'Cluster': self.clusters})\n",
    "        self.results = self.results.merge(self.landiq, left_on='UniqueID', right_on='UniqueID')\n",
    "    \n",
    "    def run_mapping(self, features): # add self. where needed\n",
    "        \n",
    "        # Convert the data type of the common column in the pandas DataFrame\n",
    "        features[\"UniqueID\"] = features[\"UniqueID\"].astype(self.results[\"UniqueID\"].dtype)\n",
    "        \n",
    "        # Join the LandIQ geodataframe and with the clustering results\n",
    "        joined = features.merge(self.results, on=\"UniqueID\")\n",
    "     \n",
    "        # Plot the fields \n",
    "        fig, ax = plt.subplots()\n",
    "        features.plot(ax=ax, color='grey', label='Other')\n",
    "        unique_values = joined['Cluster'].unique()\n",
    "        if num_clusters <= 5:\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple'][:num_clusters]\n",
    "        else:\n",
    "            colors = cm.get_cmap('Spectral', num_clusters)\n",
    "        for value in unique_values:\n",
    "            subset = joined[joined['Cluster'] == value]\n",
    "            subset.plot(ax=ax, color=colors[value], label=value)  \n",
    "        ax.set_title('Field Boundaries')\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        #ax.grid(True)\n",
    "        if subclass_values==['**']:\n",
    "            filename = f\"cluster_map_{class_columns}_{class_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "        else: \n",
    "            filename = f\"cluster_map_{class_columns}_{class_values}_{subclass_columns}_{subclass_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "        plt.savefig(filename)        \n",
    "        plt.show()\n",
    "        \n",
    "        return joined\n",
    "    \n",
    "    def run_graphing(self):\n",
    "        \n",
    "        # Days of the year\n",
    "        date_object = datetime.datetime.strptime(self.startdate, '%Y-%m-%d')\n",
    "        year = date_object.year\n",
    "        if year == 2020:\n",
    "            days_in_month = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        else:\n",
    "            days_in_month = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "        doy = np.cumsum(days_in_month)\n",
    "        \n",
    "        # Calculate the sum and mean of the ET data points in each cluster\n",
    "        max_cluster = np.max(self.clusters)\n",
    "        cluster_sums = []\n",
    "        cluster_means = []\n",
    "        for cluster in range(max_cluster + 1):\n",
    "            cluster_sum = np.sum(self.reshaped[self.clusters == cluster], axis=(1, 2))\n",
    "            cluster_mean = np.mean(cluster_sum)\n",
    "            cluster_sums.append(cluster_sum)\n",
    "            cluster_means.append(cluster_mean)\n",
    "        print(\"Cluster mean:\", cluster_means)\n",
    "                 \n",
    "        # Plotting each cluster's ET data points on top in different colors\n",
    "        cluster_counts = [np.sum(self.clusters == cluster_idx) for cluster_idx in range(num_clusters)]\n",
    "        sorted_clusters = np.argsort(cluster_counts)[::-1]\n",
    "        fig, ax2 = plt.subplots()\n",
    "        if num_clusters <= 5:\n",
    "            colors = ['red', 'blue', 'green', 'orange', 'purple'][:num_clusters]\n",
    "        else:\n",
    "            colors = cm.get_cmap('Spectral', num_clusters)\n",
    "        for cluster_idx in sorted_clusters:\n",
    "            cluster_indices = np.where(self.clusters == cluster_idx)[0]\n",
    "            for idx in cluster_indices:\n",
    "                if varColumn=='et' or varColumn=='et_ensemble_mad':\n",
    "                    ax2.plot(doy, self.reshaped[idx].squeeze(), color=colors[cluster_idx], linewidth=0.25)\n",
    "                else: \n",
    "                    ax2.plot(self.timeseries[dateColumn].unique(), self.reshaped[idx].squeeze(), color=colors[cluster_idx], linewidth=0.25)\n",
    "        months = range(1, 13)\n",
    "        month_labels = [datetime.date(2000, month, 1).strftime('%b') for month in months]\n",
    "        if varColumn=='et' or varColumn=='et_ensemble_mad':\n",
    "            ax2.set_xticks(doy)\n",
    "            ax2.set_xticklabels(month_labels)\n",
    "        else: \n",
    "            ax2.set_xticklabels(self.timeseries[dateColumn].unique())    \n",
    "        ax2.set_xlabel(dateColumn)            \n",
    "        ax2.set_title(\"All Points\")\n",
    "        ax2.set_ylabel(varColumn)\n",
    "        plt.tight_layout()\n",
    "        if subclass_values==['**']:\n",
    "            filename = f\"cluster_plot_{class_columns}_{class_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "        else: \n",
    "            filename = f\"cluster_plot_{class_columns}_{class_values}_{subclass_columns}_{subclass_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "\n",
    "        # Contingency table between clusters and main crop \n",
    "        ct_maincrop = pd.crosstab(self.results['Cluster'],self.results['MAIN_CROP'])\n",
    "        print(ct_maincrop)\n",
    "        \n",
    "        # Contingency table between clusters and all multi-cropping (info in LIQ_REPORT)\n",
    "        ct_multicropping = pd.crosstab(self.results['Cluster'],self.results['LIQ_REPORT'])\n",
    "        print(ct_multicropping)\n",
    "        \n",
    "        # Convert cluster results to a numpy array\n",
    "        clusters = np.array(self.results['Cluster'])\n",
    "        \n",
    "        if self.varColumn=='et' or varColumn=='et_ensemble_mad':\n",
    "            # Calculate cumulative ET for each UniqueID\n",
    "            cumulative_et = self.reshaped.cumsum(axis=1)\n",
    "            \n",
    "            # Calculate derivative ET for each UniqueID\n",
    "            derivative_et = np.diff(self.reshaped, axis=1)\n",
    "    \n",
    "            # Graphing the cumulative ET\n",
    "            fig, ax = plt.subplots()\n",
    "            cluster_counts = [np.sum(self.clusters == cluster_idx) for cluster_idx in range(num_clusters)]\n",
    "            sorted_clusters = np.argsort(cluster_counts)[::-1]\n",
    "            if num_clusters <= 5:\n",
    "                colors = ['red', 'blue', 'green', 'orange', 'purple'][:num_clusters]\n",
    "            else:\n",
    "                colors = cm.get_cmap('Spectral', num_clusters)\n",
    "            for cluster_idx in sorted_clusters:\n",
    "                cluster_indices = np.where(clusters == cluster_idx)[0]\n",
    "                for idx in cluster_indices:\n",
    "                    ax.plot(doy, cumulative_et[idx], color=colors[cluster_idx], linewidth=0.25)\n",
    "            months = range(1, 13)\n",
    "            month_labels = [datetime.date(2000, month, 1).strftime('%b') for month in months]\n",
    "            ax.set_xticks(doy)\n",
    "            ax.set_xticklabels(month_labels)\n",
    "            ax.set_title(\"All Points\")\n",
    "            ax.set_ylabel('Cumulative ET (mm)')\n",
    "            plt.tight_layout()\n",
    "            if subclass_values==['**']:\n",
    "                filename = f\"cum_ET_plot_{class_columns}_{class_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            else: \n",
    "                filename = f\"cum_ET_plot_{class_columns}_{class_values}_{subclass_columns}_{subclass_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            plt.savefig(filename)\n",
    "            plt.show()\n",
    "            \n",
    "            # Graphing the derivative ET\n",
    "            fig, ax1 = plt.subplots()\n",
    "            cluster_counts = [np.sum(self.clusters == cluster_idx) for cluster_idx in range(num_clusters)]\n",
    "            sorted_clusters = np.argsort(cluster_counts)[::-1]\n",
    "            if num_clusters <= 5:\n",
    "                colors = ['red', 'blue', 'green', 'orange', 'purple'][:num_clusters]\n",
    "            else:\n",
    "                colors = cm.get_cmap('Spectral', num_clusters)\n",
    "            for cluster_idx in sorted_clusters:\n",
    "                cluster_indices = np.where(clusters == cluster_idx)[0]\n",
    "                for idx in cluster_indices:\n",
    "                    if num_clusters <= 5:\n",
    "                        ax1.plot(doy[1:], derivative_et[idx], color=colors[cluster_idx], linewidth=0.25)\n",
    "                    else:\n",
    "                        ax1.plot(doy[1:], derivative_et[idx], color=colors[cluster_idx], linewidth=0.25)\n",
    "            months = range(1, 13)\n",
    "            month_labels = [datetime.date(2000, month, 1).strftime('%b') for month in months]\n",
    "            ax1.set_xticks(doy)\n",
    "            ax1.set_xticklabels(month_labels)\n",
    "            ax1.set_title(\"All Points\")\n",
    "            ax1.set_ylabel('Derivative ET (mm)')\n",
    "            \n",
    "            # Add a horizontal line at y=0\n",
    "            ax1.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            if subclass_values==['**']:\n",
    "                filename = f\"derivative_ET_plot_{class_columns}_{class_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            else: \n",
    "                filename = f\"derivative_ET_plot_{class_columns}_{class_values}_{subclass_columns}_{subclass_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            plt.savefig(filename)\n",
    "            plt.show()\n",
    "            \n",
    "            ##### Plotting the NDVI by ET Cluster #####\n",
    "            \n",
    "            # Extract UniqueID and Cluster from self.results\n",
    "            cluster_data = self.results[['UniqueID', 'Cluster']]\n",
    "\n",
    "            # Map cluster assignments to sentinel DataFrame\n",
    "            sentinel_clustered = self.sentinel.merge(cluster_data, on='UniqueID')\n",
    "\n",
    "            # Group the sentinel DataFrame by UniqueID and Cluster\n",
    "            grouped_sentinel = sentinel_clustered.groupby(['UniqueID', 'Cluster'])\n",
    "\n",
    "            # Plot a timeseries line for each group\n",
    "            fig4, ax4 = plt.subplots()\n",
    "            for (unique_id, cluster), group in grouped_sentinel:\n",
    "                color = colors[cluster] if num_clusters <= 5 else colors(cluster / num_clusters)  # Using colormap for more clusters\n",
    "                ax4.plot(np.array(group[self.dateColumn]), np.array(group['NDVI']), color=color, label=f'Cluster {cluster} - ID {unique_id}', linewidth=0.25)\n",
    "                ax4.set_title('NDVI Time Series for Each UniqueID by Cluster')\n",
    "                unique_dates = sentinel[dateColumn].unique()\n",
    "                display_dates = unique_dates[::4]\n",
    "                ax4.set_xticks(display_dates)\n",
    "                ax4.set_xticklabels(display_dates, rotation=45)\n",
    "                ax4.set_xlabel('Date')\n",
    "                ax4.set_ylabel('NDVI')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            if subclass_values==['**']:\n",
    "                filename = f\"NDVI_using_ET_clusters_plot_{class_columns}_{class_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            else: \n",
    "                filename = f\"NDVI_using_ET_clusters_plot_{class_columns}_{class_values}_{subclass_columns}_{subclass_values}_{startdate}_{enddate}_{varColumn}_{idColumn}_{dateColumn}_number_of_clusters_{num_clusters}.png\"\n",
    "            plt.savefig(filename)\n",
    "            plt.show()\n",
    "            print('ran fig4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dafe1c",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LandIQ data\n",
    "landiq = pd.read_csv(r'C:\\Users\\kdrechsler2\\Box\\Valley_Water\\Tables\\LandIQ\\i15_Crop_Mapping_2020_Santa_Clara_Table.csv')\n",
    "\n",
    "# Load the timeseries data\n",
    "timeseries = pd.read_csv(r'C:\\Users\\kdrechsler2\\Box\\Valley_Water\\Tables\\OpenET\\2020_OpenET_zonal_statistics_Ensemble.csv')\n",
    "#timeseries = pd.read_csv(r'C:\\Users\\kdrechsler2\\Box\\Valley_Water\\Tables\\Sentinel-2\\VW_zonal_statistics_2019-2021_mean_uniqueID_copy.csv')\n",
    "\n",
    "# Load the sentinel timeseries data\n",
    "sentinel = pd.read_csv(r'C:\\Users\\kdrechsler2\\Box\\Valley_Water\\Tables\\Sentinel-2\\VW_zonal_statistics_2019-2021_mean_uniqueID_copy.csv')\n",
    "\n",
    "# Import the CSV file into a DataFrame\n",
    "cloudydates = pd.read_csv('unique_dates_cloudy_50percent.csv')\n",
    "cloudydates['date'] = pd.to_datetime(cloudydates['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb56c1",
   "metadata": {},
   "source": [
    "Define the parameters for the TimeSeriesAnalysis instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_columns = ['CLASS1', 'CLASS2', 'CLASS3', 'CLASS4']\n",
    "subclass_columns = ['SUBCLASS1', 'SUBCLASS2', 'SUBCLASS3', 'SUBCLASS4']\n",
    "class_values = [' V'] # IMPORTANT: include any required leading spaces\n",
    "subclass_values = ['**'] # Put ['**'] if blank. REMEMBER leading spaces.\n",
    "startdate = '2020-01-01' # Start date of the analysis\n",
    "enddate = '2020-12-31' # End date of the analysis\n",
    "varColumn = 'et_ensemble_mad' # Column name of the timeseries variable\n",
    "idColumn = 'UniqueID' # Column name of the unique field ID from LandIQ\n",
    "dateColumn = 'date' # Column name of the dates\n",
    "num_clusters = 2 # Number of clusters for the clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34db30",
   "metadata": {},
   "source": [
    "Filter sentinel data by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650e59ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel = sentinel[(sentinel['date'] >= startdate) & (sentinel['date'] <= enddate)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0cea6f",
   "metadata": {},
   "source": [
    "Create an instance of TimeSeriesAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d534c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = TimeSeriesAnalysis(timeseries, \n",
    "                              landiq, \n",
    "                              sentinel,\n",
    "                              class_columns, \n",
    "                              subclass_columns, \n",
    "                              class_values, \n",
    "                              subclass_values, \n",
    "                              startdate, \n",
    "                              enddate, \n",
    "                              varColumn, \n",
    "                              idColumn, \n",
    "                              dateColumn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad55a20",
   "metadata": {},
   "source": [
    "Run the analysis using the specified parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd60d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.run_analysis(num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7d2d0",
   "metadata": {},
   "source": [
    "Mapping and graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18f458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile\n",
    "shapefile_path = r'C:\\Users\\kdrechsler2\\Box\\Valley_Water\\Shapefiles\\i15_Crop_Mapping_2019_Santa_Clara.shp'\n",
    "features = gpd.read_file(shapefile_path)\n",
    "joined = analysis.run_mapping(features)\n",
    "\n",
    "# Reshape the data for graphing\n",
    "reshaped = analysis.reshaping(cloudydates)\n",
    "\n",
    "# Other graphs\n",
    "analysis.run_graphing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
